\chapter{Current Progress}

\section{General Progress}
As indicated in the schedule, the mid-year report is a key milestone in the project. To date I have completed the Aims \& Objectives section of the report, laying out what I hope to achieve in the project, and why it is relevant. I have also completed the necessary Background Research for the project, discussing Cloud Computing and Big Data, key concepts of my project, and the technologies that I am going to be assessing in my experiments. 

I have initial ideas for the design of my experiments which are outlined below. I have also made progress with the technologies which will be required for implementation and 

Outside of the report, I have presented my project to the Distributed Systems and Services research group, outlying the idea for my project, the core questions I hope to answer and how I plan to achieve answers to those questions. The presentation was received well, and feedback was positive.

I am currently on track according to my project schedule.

\section{Experiment Design}
I plan to run a series of experiments to test Hadoop and Nephele, with the aim of answering my research questions. There will be a total of 18 experiments, with 3 factors being varied to test the tools in a range of scenarios to answer the research questions.

\subsection{The Data Processing Tool}
The first factor that will be varied is the data processing tool. Experiments will be run in 3 tool configurations.

\subsubsection{Hadoop}
Hadoop is the established tool for performing large scale data processing. It therefore acts as a reference point for acceptable levels of efficiency in data processing, and allows comparison to a real-world tool. 

Hadoop can also be considered as the reference implementation of the MapReduce programming paradigm. This makes it an ideal choice for the comparison of MapReduce to PACT.

\subsubsection{Nephele using MapReduce}
Experiments will be run using Nephele, but limiting the operations available to the MapReduce subset. This allows Nephele and Hadoop to be directly compared, as they can both process tasks using MapReduce. If Nephele can perform MapReduce tasks more efficiently than Hadoop, there may be limited value to the existing established system.

\subsubsection{Nephele using PACT}
Nephele is a tool which implements the PACT programming paradigm, which as a superset of MapReduce can also perform tasks in a MapReduce style. This means that experiments can be implemented using the same tool, but in different programming paradigms, allowing a fair test of PACT vs Nephele. This is designed to answer the first research question, namely whether or not PACT overcomes the inherent disadvantages of the MapReduce paradigm.

In a broader sense, it allows a complete comparison of Hadoop and Nephele. In real-world data processing scenarios, users of Nephele would be unlikely to restrict their programming options to just MapReduce, and would be likely to use other aspects of PACT as appropriate. Running experiments in the same way allows a fairer comparison of the 2 tools, where all of their functionality is available.

\subsection{The Problem}
Data Processing Tools are generic and can solve a wide range of problems. I plan to use the tools to solve 3 different problems, each designed to test the tools in different scenarios.

The specific problems and datasets are yet to be decided, although potential problems are listed for each type.

\subsubsection{A `Classic' MapReduce problem}
There are various problems which naturally fit to the MapReduce paradigm, and have been implemented several times to benchmark Hadoop and other MapReduce tools. The classic MapReduce problem will give a good representation of how well Nephele can perform MapReduce tasks in comparison to Hadoop.

One issue with choosing a classic MapReduce problem is that PACT does not necessarily add anything to the solution of the problem. Depending on the choice of problem, it could be reformulated to use some of the other input contracts that PACT provides. Alternatively, even if only the Map and Reduce input contracts are used, PACT output contracts could be used to potentially improve performance.

Examples of Classic MapReduce problems are word counting, a distributed text search (`grep') or Terasort.

\subsubsection{A Non-Classic MapReduce problem}
It is well documented that there are various problems which are difficult to reformulate into the MapReduce paradigm. This is often cited as one of the major disadvantages of MapReduce. As these problems still need to be solved in Big Data situations, a sub-optimal use of the MapReduce paradigm is often used. This provides an opportunity to test whether the extra flexibility of the PACT programming model translates into actual benefits, and whether it overcomes the disadvantages of MapReduce. 

Problems that MapReduce are not suited to are typically iterative in nature, such as K-Means clustering.  

\subsubsection{A Real-World Problem}
Whilst there is value in testing simple problems such as word counting for benchmarks, they are artificial and may not provide a reflection of how data processing tools are used in industry and academia. A real world problem will be used to determine how well the data processing tools perform in a more realistic scenario. As a real world scenario is likely to be more complex, it provides more of an opportunity to take advantage of different aspects of the data processing software. There is rarely one `correct' way of solving a non-trivial problem, and there will therefore hopefully be ways of solving the problem which allow for PACT and MapReduce solutions developed in an idiomatic way. 

Various data sets are made publicly available. Amazon make 55 large datasets publicly available for analysis, including data from the 1000 Genomes Project (a map of human genetic variation), the NASA NEX project (various scientific datasets, including images of the Earth's surface and climate change projections) and US Census data.

As an alternative, the School of Computing has a 40GB dataset with information on Google data centres. 

I hope to find papers demonstrating existing scientific analysis on a publicly available dataset (possibly performed by methods such as MapReduce). I will then attempt to replicate the results of the analysis using my chosen data processing tools.

\subsection{Deployment Environment}
The final research question is how well the data processing tools perform in Cloud Computing environments. Hadoop is `rack-aware' - the machines used in a data processing task must be known up front, and cannot change throughout the process. One of the stated research goals of Nephele is to take maximum advantage of the elasticity that Cloud Computing affords.

In order to assess the performance of both tools in Cloud Computing environments, I will execute the experiments in 2 different environments.

\subsubsection{Cloud Testbed}
The experiments will all be run on the School of Computing cloud testbed, a private cloud operated by the Distributed Systems and Services group. This provides a static environment with well known hardware and limited elasticity. 

\subsubsection{Public Cloud}
The experiments will also be run on a public cloud, such as those provided by Amazon or Microsoft. In a public cloud environment, the hardware is not known and could be of varying quality. This will show whether designing Nephele to run on the cloud, and to take advantage of the elasticity and available resources, translates in to real world benefits.

\section{Experiment Implementation}
Experiments will be implemented using the Scala programming language. Scala has been chosen for the following reasons.

\begin{itemize}
    \item All experiments should be implemented in the same programming language to maximise code reuse, and allow for easier comparison of implementations.
    \item Nephele has limited support for programming languages, whereas Hadoop (as a more mature project) supports many more. Nephele currently only supports Java and Scala. The Scala API for Nephele currently has superior documentation.
    \item Code examples illustrating Hadoop and Nephele tasks are substantially terser than the Java equivalents.
\end{itemize}

I have implemented common introductory Hadoop exercises using Scala and have executed them on a local emulator. These basic implementations use an Open Source Hadoop library provided by Twitter (called Scalding).

I have received access to the School of Computing Cloud Testbed, and have carried out introductory exercises on provisioning Virtual Machines. There are currently some issues with the Hadoop Virtual Machine, but I hope these will be resolved in the near future so I can run tests in the environment the experiments will be executed in.

\section{Evaluation}
I plan to evaluate the project on a number of factors.

\subsection{Technical Evaluation}
The technical evaluation will evaluate the results of the experiment, and determine what the results of the experiment mean. The aim of the technical evaluation will be to answer the research questions. Key to this is the definition of efficiency which is used to evaluate the tools. There are a number of types of efficiency which could be used to evaluate the project.

The tools will primarily be evaluated on Runtime Efficiency, though there are various alternative metrics which could be used.

\begin{itemize}
    \item \textbf{Runtime Efficiency}
    Runtime efficiency is the length of time that it takes to complete a data processing job. In order to determine runtime, each experiment will be run multiple times with each individual runtime being averaged. This will minimise discrepancies caused by hardware, over-utilisation of resources, etc. 

    The runtime efficiency is an important factor because completing data processing tasks in a timely manner is one of the largest considerations in Big Data scenarios. This will form the bulk of the technical evaluation into how efficient the tools are.

    \item \textbf{Resource Utilisation}
    The tools should use resources in the best possible way. If a data processing tool is only using half of the possible resources available to it on a virtual machine, it may explain why it is less efficient (in regards to runtime) than an alternative tool. Likewise, excessive use of disk I/O may lead to a slower solution.

    \item \textbf{Scalability}
    It is essential to use distributed computing to process large amounts of data. An important consideration in this environment would be how adding extra machines improves the runtime efficiency of a solution. If one tool scales much better than an alternative, it may be a more suitable choice for big data situations, even if the alternative has a better runtime efficiency when a small number of machines are used. 

    \item \textbf{Cost}
    One of the major factors of Cloud Computing is cost. Resources are charged on a pay-per-usage model, so the more resources that are allocated for data processing, the higher the cost will be. An evaluation can be performed of the cost of resources in comparison to the amount of work completed. 

    An evaluation of cost would take in to account the cost of cloud computing compute time to the amount of time it takes to complete a task. Cloud Service Providers such as Microsoft and Amazon offer hosted Hadoop platforms (as a Platform as a Service offering), and the cost of this could be compared to performing data processing tasks on Infrastructure as a Service offerings (creating a set of virtual machines with Hadoop). The scalability of a data processing tool will be relevant to the cost (at what point does it become more expensive to add new virtual machines than the benefit it provides?). The cost of using the cloud could also be compared to the relative cost of purchasing hardware and setting up a private cloud (or merely creating a static data processing data centre). 
\end{itemize}

\subsection{Process Evaluation}
The process evaluation will evaluate the process of the project as a whole, rather than considering the results of the experiments. The process will also be evaluated through a number of factors.

\begin{itemize}
    \item Does the technical evaluation match the hypothesis?
    \item Have the research questions been answered?
    \item Does the work contribute to the existing research landscape?
    \item Is there any further scope for research in this area?
\end{itemize}